{
	"jobConfig": {
		"name": "etlenergia",
		"description": "etl para hacer una transformacion de  CSV en S3 en formato parquet",
		"role": "arn:aws:iam::016442247674:role/service-role/AWSGlueServiceRole-carlos",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "etlenergia.py",
		"scriptLocation": "s3://aws-glue-assets-016442247674-us-east-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-07-29T18:45:31.557Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-016442247674-us-east-2/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-016442247674-us-east-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom pyspark.sql.functions import col\r\nfrom awsglue.dynamicframe import DynamicFrame\r\n\r\n\r\n\"\"\"\r\n    Este código lee desde el AWS Glue Datacatalog dos tablas en una base de datos, después realiza un join entre ambas tablas y calcula el total de la venta. \r\n    Finalmente, escribe el resultado en un archivo Parquet en S3. \r\n\"\"\"\r\n\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\n\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\ndyf_transacciones = glueContext.create_dynamic_frame.from_catalog(\r\n    database=\"dbbronce\", \r\n    table_name=\"transacciones\",\r\n    transformation_ctx=\"dyf_transacciones\"\r\n)\r\n\r\ndyf_clientes = glueContext.create_dynamic_frame.from_catalog(\r\n    database=\"dbbronce\",\r\n    table_name=\"clientes\",\r\n    transformation_ctx=\"dyf_clientes\"\r\n)\r\n\r\ndf_transacciones = dyf_transacciones.toDF()\r\ndf_clientes = dyf_clientes.toDF()\r\ndf_transacciones = df_transacciones.withColumn(\"energia_kwh\", col(\"energia_kwh\").cast(\"int\"))\r\ndf_transacciones = df_transacciones.withColumn(\"price\", col(\"precio_total_usd\").cast(\"float\"))\r\n   \r\ndf_joined = df_transacciones.join(df_clientes, on=\"id_cliente\", how=\"inner\")\r\n\r\ndf_transformed = df_joined.withColumn(\"total_amount\", col(\"energia_kwh\") * col(\"precio_total_usd\"))\r\n\r\ndyf_transformed = DynamicFrame.fromDF(df_transformed, glueContext, \"dyf_transformed\")\r\n\r\nglueContext.write_dynamic_frame.from_options(\r\n    frame = dyf_transformed,\r\n    connection_type = \"s3\",\r\n    connection_options = {\"path\": \"s3://datawarehouse-energia-carlos\"},\r\n    format = \"parquet\",\r\n    transformation_ctx = \"write_parquet\"\r\n)\r\n\r\njob.commit()"
}